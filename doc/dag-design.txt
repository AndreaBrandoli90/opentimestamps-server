Merkle Calendars
----------------

Essentially we keep multiple trees stored with the standard linear
breadth-first method, with the largest tree starting at index 0. This array
is called 'tips', and has the interesting property that any member of that
array is the tip of a tree, although usually not a very tall tree.

Every time a digest is added to the end of the tips we see if that digest
allows us to create a larger tree. Lets define the height of the tree, h,
as representing a tree with 2^h total roots. Thus as we add digests, the
heights of the array grows as follows:

0
00
001 <- indexed 0 and 1 are hashed to form index 3
0010
0010012 <- another tree, which lead to the two subtrees being merged (height 2)
00100120
0010012001 <- now we have two trees, one of height 2, one of height 1

Now when we sign the tree, we just take the tips of every subtree, hash
them together in a merkle tree, and sign that. That hash can be
deterministicly calculated just by knowing the length of the tips array
when the signature was created. Similarly because every signature is
applied to every digest submitted before the signature was created, if the
client knows the index of their digest, searching for signatures just
becomes a fast binary search in a per-method list of signatures, sorted by
tips length. Fast!

However, all this assumes there is one, and only one calendar.


Divergent Trees
---------------

Suppose we want have multiple servers accepting, and creating signatures for, a
single calendar. If we instead order the full list of digests submitted in some
way, such as numerically, we find that eventually all servers will come to
agreement provided they all have the same list of digests. Also, sub-lists of
digests, even if the full list isn't available, are also sorted the same way.
Thus sub-trees created from parts of the known list will be identical!

You can efficiently build up trees iteratively in this way. For each new digest
put it in the right order, then build up the new appropriate tree. For a random
order you'll find that at height 0, on average one hash pair is broken, thus
that incures a requirement to store an additional digest. For height 1, again
an additional. The height of the tree is proportional to log2(n), thus the
total storage requirement for all the trees built in this fashion is n*log2(n)

Having multiple servers build these trees iteratively doesn't change the result
significantly essentially because if they're fairly "up-to-date" with each
other, the vast majority of the tree will be the same. Still, I haven't thought
about this carefully.

From a theoretical perspective this is great, but from a practical point of
view needing 10 to 30 times more storage isn't exactly very good. The other
problem is that essentially your tree now has a "rectangular" shape, so finding
what signatures are available for what digests becomes tricky again.

If you can find an ordering where "chunks" of additional digests are unlikely
to "break up" existing trees you can get around that problem, modulo the fact
that a malicious server can break up your ordering. (although some degree of
trust probably exists) Time itself doesn't work, because the general case has
multiple servers accepting digests with very similar times. You can just pick a
random number, usually known as a UUID, to identify each server, but
essentially you've just pushed the n*log(n) scaling up a level. You also have
the same problems with signature lookup. This still might be a reasonable
solution, but no sense rushing to implement it.
